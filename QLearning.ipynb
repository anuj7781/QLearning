{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l7c5L9ClU_m",
        "colab_type": "text"
      },
      "source": [
        "# How To Use the Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olh7HxOulU_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym \n",
        "import random \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Z3k8brlU_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#instantiate the enviornment\n",
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsyYx9oTlU_z",
        "colab_type": "code",
        "colab": {},
        "outputId": "cbb6d758-a73c-4609-c8b7-b04cfdd8e05b"
      },
      "source": [
        "#observation-space : what are the metrics/parameters on the basis of which represent the state\n",
        "print(env.observation_space)#position velocity angular-velocity angle\n",
        "\n",
        "print(env.action_space)\n",
        "\n",
        "print(env.action_space.n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(4,)\n",
            "Discrete(2)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L_3OoYOlU_5",
        "colab_type": "code",
        "colab": {},
        "outputId": "9c0baacd-2ac8-4f66-ca7a-f5d3a738730b"
      },
      "source": [
        "#it returns the initial state\n",
        "state = env.reset();\n",
        "\n",
        "for _ in range(200):\n",
        "#     choose some action randomly\n",
        "#     action = env.action_space.sample()\n",
        "#     action_size = env.action_space.n\n",
        "#     action = random.choice(range(action_size))\n",
        "    \n",
        "#     we will choose the action wisely if pole is moving right push the cart right and similarly for the other case\n",
        "    pole_angle = state[2]\n",
        "    #this is the policy on the basis of which we select the action\n",
        "    action = 0 if pole_angle < 0 else 1\n",
        "    #apply the action\n",
        "    state,reward,done,info = env.step(action)\n",
        "    \n",
        "    env.render()#render means display\n",
        "    \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/hp/anaconda3/lib/python3.5/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InpL7qGblU_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WiG3kqAlVAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#another example where the action space is continious\n",
        "env_name = 'MountainCarContinuous-v0'\n",
        "# to create the new enviornment\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huq7yN7GlVAD",
        "colab_type": "code",
        "colab": {},
        "outputId": "d7021e35-c2d6-4601-e6c2-0f5073282dec"
      },
      "source": [
        "print(env.observation_space)#two parameters\n",
        "print(env.action_space)#only one action(continious)\n",
        "print(env.action_space.low,env.action_space.high)#force applied to the car between -1 to 1\n",
        "print(env.action_space.shape)#only one action passed at a time "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(2,)\n",
            "Box(1,)\n",
            "[-1.] [1.]\n",
            "(1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mc4GmexlVAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "for _ in range(200):\n",
        "    action = env.action_space.sample()#choose a value randomly between -1 to 1\n",
        "    state,reward,done,info = env.step(action)\n",
        "    env.render()\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEy3b5yvlVAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhCxIXiXlVAP",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcment Learning(Q-Learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waHNy68dlVAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reinforcment learning lies between supervised and unsupervised we don't tell which action to take(whether to go left or right) but tell that a sequence of actions(left right left) was a good path or not\n",
        "\n",
        "#q-learning we are going to maintain a table(q-table)\n",
        "\n",
        "#frozen-lake state is the cell where the agent currently is,actions are up,down,left,right\n",
        "#in case of 4*4 matrix  we have 16 states\n",
        "#in q-table we maintain all the states(16) and for each state we maintain the reward for all the actions(4) and we choose the action which provides the maximum reward in a state\n",
        "\n",
        "#our task is to fill the q-table\n",
        "#we have to find how much total reward we will get if we take a action(go down by that path)\n",
        "\n",
        "#we have to optimise the total-reward not the immediate-reward only\n",
        "#you have the immediate reward plus the reward for the rest of the journey till the end(there is uncertainity as we are looking into the future now)\n",
        "\n",
        "#the impact of the reward will reduce the more we go into the future(beacuse the uncertainity of recieving the reward increases)\n",
        "#this  is called discount the reward \n",
        "\n",
        "#the aim of q-learning is to maximise the total  expected reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49WxBqoElVAT",
        "colab_type": "text"
      },
      "source": [
        "# Q(st,at) =  rt+1 + G.Qmax(st+1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jay9Df-lVAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFobG9gplVAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.registration import register\n",
        "#to make the game non slippery\n",
        "try:\n",
        "    register(\n",
        "        id='FrozenLake-v0',\n",
        "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "        kwargs={'map_name' : '4x4','is_slippery':False},\n",
        "        max_episode_steps=100,\n",
        "        reward_threshold=0.78, # optimum = .8196\n",
        "    )\n",
        "except:\n",
        "    pass\n",
        "\n",
        "env_name = 'FrozenLake-v0'\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kLhRdC-olVAd",
        "colab_type": "code",
        "colab": {},
        "outputId": "750bac89-21ca-4911-b1f9-abd7cb6c77bc"
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "\n",
        "type(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(16)\n",
            "Discrete(4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gym.spaces.discrete.Discrete"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEZ9ygfmlVAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the agent should work for both discrete and continious games"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8w4vToJlVAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we define an agent class to create an agent\n",
        "class Agent():\n",
        "    #constructor\n",
        "    def __init__(self,env):\n",
        "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
        "        if self.is_discrete:\n",
        "            self.action_size = env.action_space.n\n",
        "        else:\n",
        "            self.action_low = env.action_space.low\n",
        "            self.action_high = env.action_space.high\n",
        "            self.action_shape = env.action_space.shape\n",
        "        \n",
        "    #we will call this function to figure out which action should we take next\n",
        "    #so all the logic will be in this function\n",
        "    def get_action(self,state):\n",
        "        # action = env.action_space.sample()\n",
        "        if self.is_discrete:\n",
        "            action = random.choice(range(self.action_size))\n",
        "        else:\n",
        "            action = np.random.uniform(self.action_low,self.action_high,self.action_shape)\n",
        "        return action   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cFXSSz3lVAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = Agent(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqo9mL0YlVAu",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c7aaa9b-3e4e-4f17-e8ba-84a0052089d9"
      },
      "source": [
        "#everytime you play a game is called an episode    \n",
        "for ep in range(2):\n",
        "    #initial state\n",
        "    state = env.reset()#0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state,reward,done,info = env.step(action)\n",
        "        env.render()#we dont have to close as it will render on the screen itself\n",
        "        time.sleep(0.05)\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvM93iDelVA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1epa_6xBlVA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class QAgent inherits Agent class\n",
        "#learning rate is kept small to avoid fluctuations\n",
        "class QAgent(Agent):\n",
        "    def __init__(self,env,discount_rate=0.97,learning_rate=0.01):\n",
        "        super().__init__(env)\n",
        "        self.state_size = env.observation_space.n\n",
        "        #most of the times we will choose the action with the max q-value to maximise the reward\n",
        "        #but in some cases we will choose a random action to explore a better path that may be available\n",
        "        self.exploration_rate = 1.0 #epsilon\n",
        "        #at start the exploration rate is 1 because all the q values are random\n",
        "        #after every episode exploration rate will be reduced, as q-values start becoming meaningful\n",
        "        #as we are learning q-value we will reduce over tendency to explore\n",
        "        self.discount_rate = discount_rate #gamma\n",
        "        self.learning_rate  =learning_rate #alpha\n",
        "        self.q_table = 1e-4 * np.random.random([self.state_size,self.action_size])\n",
        "        \n",
        "    def get_action(self,state):\n",
        "        q_state = self.q_table[state]\n",
        "        #action chosen on basis of q_value\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        #action chosen randomly\n",
        "        action_random = super().get_action(state)\n",
        "        #need to choose between these two actions\n",
        "        return action_random if random.random()<self.exploration_rate else action_greedy\n",
        "    \n",
        "    def train(self,experience):\n",
        "        state,action,next_state,reward,done = experience\n",
        "        \n",
        "        q_next = self.q_table[next_state]\n",
        "    \n",
        "        #if you have reached the goal state then there is no expected reward in future\n",
        "        q_next = np.zeros([self.action_size]) if done else q_next\n",
        "        \n",
        "        # total_reward = immediate_reward + expected_reward in future\n",
        "        q_target = reward + self.discount_rate * np.max(q_next)\n",
        "        \n",
        "        q_update = q_target - self.q_table[state,action] #expected reward calculated - orignal value in q_table\n",
        "        \n",
        "        self.q_table[state,action] += self.learning_rate*q_update\n",
        "        \n",
        "        #when one episode is completed we reduce our tendecy to explore\n",
        "        if done:\n",
        "            self.exploration_rate *= 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuxD08GflVBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = QAgent(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMPrX67slVBD",
        "colab_type": "code",
        "colab": {},
        "outputId": "c4da904c-5368-4776-83c0-92253f1d389f"
      },
      "source": [
        "total_reward = 0\n",
        "for ep in range(100):\n",
        "    #initial state\n",
        "    state = env.reset()#0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state,reward,done,info = env.step(action)\n",
        "        agent.train((state,action,next_state,reward,done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        print(\"s: \",state,\" a: \",action)\n",
        "        print(\"Episode: \",ep,\" Reward: \",total_reward,\" Explore: \",agent.exploration_rate)\n",
        "        \n",
        "        env.render()\n",
        "        \n",
        "        print(agent.q_table)\n",
        "        \n",
        "        time.sleep(0.05)\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:  12  a:  1\n",
            "Episode:  99  Reward:  1.0  Explore:  0.358748297681892\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "[[3.68661318e-05 6.75374788e-05 6.74436098e-05 6.47374251e-05]\n",
            " [4.75017310e-05 2.24187138e-05 2.59189751e-05 2.20851530e-05]\n",
            " [6.93681163e-05 2.67686178e-05 1.23655433e-05 1.52886390e-05]\n",
            " [1.85738434e-05 5.12426286e-05 7.24489014e-05 6.01941592e-05]\n",
            " [1.77149480e-05 4.59608759e-05 6.77874757e-05 2.12447639e-05]\n",
            " [6.07175591e-06 1.65637967e-05 9.10221060e-05 2.70826131e-06]\n",
            " [3.51067111e-05 1.68051713e-05 4.78199258e-05 7.94139641e-05]\n",
            " [3.07400426e-05 8.32873674e-05 7.55470320e-05 7.75336445e-05]\n",
            " [5.87015244e-05 7.58361437e-05 8.34480507e-05 7.87326332e-05]\n",
            " [6.10815839e-05 1.89020577e-05 1.00252699e-05 3.34534796e-05]\n",
            " [8.10621473e-05 2.35744603e-05 5.21444799e-05 7.29577279e-05]\n",
            " [2.15004943e-05 3.51842748e-05 8.19211404e-05 7.21198541e-05]\n",
            " [8.44808018e-05 5.23584787e-05 7.24382893e-05 9.96305006e-05]\n",
            " [7.65358137e-05 1.31739669e-05 3.35608191e-05 7.41795469e-05]\n",
            " [6.45046588e-05 1.00681720e-02 3.62209503e-05 4.81621968e-05]\n",
            " [7.58266704e-05 1.30582626e-05 4.44643729e-05 4.94990548e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkl0zFcTlVBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in a good training algorithm you should give it a negative reward for each step to reduce the number of steps to reach the goal state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMQeeSUulVBK",
        "colab_type": "text"
      },
      "source": [
        "# Using Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CIB_clIlVBK",
        "colab_type": "text"
      },
      "source": [
        "last time we did q-learning manually(updated the qtable manually)\n",
        "now we will implement q-learning using neural-networks\n",
        "so the q-values will become weights and neural network will optimise them\n",
        "earlier in each iteration only one value was getting updated but now multiple values will get updated so it will be faster\n",
        "and further optimisation can be done by multiple times training on previous experiences called experience relay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rufyDvU2lVBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5WvCLWtlVBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the input to the neural network will be the state and the output will be the action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfSmuaXvlVBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QNAgent(Agent):\n",
        "    def __init__(self,env,discount_rate=0.97,learning_rate=0.01):\n",
        "        super().__init__(env)\n",
        "        self.state_size = env.observation_space.n\n",
        "        self.exploration_rate = 1.0\n",
        "        self.discount_rate = discount_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.build_model()\n",
        "        \n",
        "        #we have to start running the model after we have created it\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        \n",
        "    def build_model(self):\n",
        "        \n",
        "        #so that we can assign name to layers in tf\n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        #declaring the type of variables\n",
        "        self.state_in = tf.placeholder(tf.int32,shape=[1])\n",
        "        self.action_in = tf.placeholder(tf.int32,shape=[1])\n",
        "        self.target_in = tf.placeholder(tf.float32,shape=[1])\n",
        "        \n",
        "        #one hot encoding \n",
        "        self.state = tf.one_hot(self.state_in,depth=self.state_size)#16\n",
        "        self.action = tf.one_hot(self.action_in,depth=self.action_size)#4\n",
        "        \n",
        "        #a dense layer with units = number_of_actions\n",
        "        #the result of the dense layer will be a row of the q-table\n",
        "        #output of the layer is one-hot encoded\n",
        "        self.q_state = tf.layers.dense(self.state,units=self.action_size,name='q_table')\n",
        "        \n",
        "        #then we multiply the output of the dense layer with the actual action (vector of size 4)\n",
        "        #q_action will be a scalar because we are doing reduce_sum on a vector of size 4\n",
        "        self.q_action = tf.reduce_sum(tf.multiply(self.q_state,self.action),axis=1)\n",
        "        \n",
        "        \n",
        "        #squaring and adding the errors\n",
        "        self.loss = tf.reduce_sum(tf.square(self.target_in-self.q_action))\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "    def get_action(self,state):\n",
        "        #in this no backprop is there only one feed forward(prediction)\n",
        "        #we give a state as input and ask for predicted q-values for that state\n",
        "        q_state = self.sess.run(self.q_state,feed_dict={self.state_in:[state]})\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = super().get_action(random)\n",
        "        return action_random if random.random()<self.exploration_rate else action_greedy\n",
        "    \n",
        "    #experience is a tuple with multiple values\n",
        "    def train(self,experience):\n",
        "        #list comphrension all the values on lhs are list\n",
        "        state,action,next_state,reward,done =  [[exp]for exp in experience]\n",
        "        \n",
        "        #why we are doing this???? essentially we just need to give the current_state as input and get the action as output from dense layer\n",
        "        #but also we need to realise how are we getting the q_value for the current_state\n",
        "        #current_state reward + discount * max q_value of next state\n",
        "        #so we need to get the q-values for the next state but now we dont have a table\n",
        "        #those q-values are in the dense layer and this is how we get them\n",
        "        #no training i.e backprop in this case as well\n",
        "        q_next = self.sess.run(self.q_state,feed_dict={self.state_in:next_state})\n",
        "        \n",
        "        #edge case next state wont exist if the game is over\n",
        "        #q_next = np.zeros([self.action_size]) if done else q_next\n",
        "        q_next[done] = np.zeros([self.action_size])\n",
        "        \n",
        "        q_target = reward + self.discount_rate * np.max(q_next)\n",
        "        \n",
        "        #training is done in this case the above two were only predictions\n",
        "        #for a given state if i take a action what is the predicted q-value and i also have the target q-value\n",
        "        feed = {self.state_in:state,self.action_in:action,self.target_in:q_target}\n",
        "        self.sess.run(self.optimizer,feed_dict=feed)\n",
        "        \n",
        "        #after every episode reduce your exploration_rate\n",
        "        if(done[0]):\n",
        "            #because done is a list now\n",
        "            self.exploration_rate *= 0.99\n",
        "          \n",
        "    #destructor \n",
        "    def __del__(self):\n",
        "        self.sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFx-a7UulVBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "755239c6-211f-4fae-e5d5-adddbb716576"
      },
      "source": [
        "agent = QNAgent(env)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-0cb56add1193>:33: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likeeXgQlVBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e0c02309-1826-4066-c1ed-67901409cefd"
      },
      "source": [
        "total_reward = 0.0\n",
        "for ep in range(100):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state,reward,done,info = env.step(action)\n",
        "        agent.train((state,action,next_state,reward,done))\n",
        "        state = next_state\n",
        "        total_reward += reward#in this game we get reward only when we reach the goal state\n",
        "        print(\"s: \",state,\" a: \",action)\n",
        "        print(\"Episode: \",ep,\" Reward: \",total_reward,\" Explore: \",agent.exploration_rate)\n",
        "        env.render()\n",
        "        # ReUse = True so if we run the cell again,previously learned weights in the q-table will be the start for the next iteration,i.e weights will not be reset in the next iteration when we run the cell again\n",
        "        with tf.variable_scope('q_table',reuse = True):\n",
        "            #the weights in the dense layer now represent our q-value\n",
        "            weights = agent.sess.run(tf.get_variable('kernel'))\n",
        "            print(weights)\n",
        "        time.sleep(0.05)\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:  5  a:  3\n",
            "Episode:  99  Reward:  2.0  Explore:  0.36603234127322926\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "[[ 0.04121775  0.11725765  0.10114238  0.03047478]\n",
            " [-0.07643432 -0.10581105 -0.13324344  0.02383582]\n",
            " [ 0.15284581  0.05639105  0.01886346  0.08038311]\n",
            " [ 0.03517211  0.10905992 -0.2304262   0.1074306 ]\n",
            " [ 0.11117578  0.13411562 -0.22252291 -0.0144773 ]\n",
            " [ 0.06013334  0.26583594  0.45679402  0.19534427]\n",
            " [ 0.17942011 -0.10360501 -0.11926575 -0.02127964]\n",
            " [-0.18365738  0.46836185 -0.15440989 -0.1180737 ]\n",
            " [-0.30276006 -0.07258799 -0.09086517  0.11256143]\n",
            " [-0.01686391  0.15828489  0.0325565  -0.04545236]\n",
            " [ 0.02242352  0.10428189  0.05802891  0.02220099]\n",
            " [ 0.19901901  0.04223073  0.16622734  0.28041744]\n",
            " [ 0.15605229 -0.23030052  0.0764249  -0.28423566]\n",
            " [-0.05185646 -0.3021306  -0.12070956 -0.32087773]\n",
            " [-0.09781185 -0.20094383  0.12630574  0.18454683]\n",
            " [ 0.03428936 -0.29362905 -0.0604226  -0.46508503]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CyGQUBJlVBc",
        "colab_type": "text"
      },
      "source": [
        "# QLearning + Neural Nets + Experience Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1QA69kGlVBe",
        "colab_type": "text"
      },
      "source": [
        "there are few paths that take you to the goal and a lot of paths that take you to the hole\n",
        "since the paths to goal are less so time of training done upon those paths will be less\n",
        "so a lot of training is done because of holes and not because of goals\n",
        "if we give negative reward to the holes then it knows that it has to avoid the holes which is not in this case\n",
        "so in order to maximise the reward, the rare paths upon which we are not able to go more number of times,we want to train upon those paths more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJZCPf9FlVBg",
        "colab_type": "text"
      },
      "source": [
        "This will allow us to train on rare experiences more often.\n",
        "\n",
        "1. Take a buffer. Keep adding new experiences into the buffer, and remove old ones.\n",
        "2. Take a sample of experiences from the buffer to train on.\n",
        "\n",
        "You will notice that we reach the Goal more often now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AYVSxTHlVBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7c450158-0205-4732-c419-ebbc38e3214d"
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "print(\"Using OpenAI Gym:\", gym.__version__)\n",
        "print(\"Using Tensorflow:\", tf.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using OpenAI Gym: 0.10.11\n",
            "Using Tensorflow: 1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOPppHM2lVBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QNRAgent(Agent):\n",
        "    def __init__(self, env, discount_rate=0.97, learning_rate=0.001): # Smaller learning rate\n",
        "        super().__init__(env)\n",
        "        self.state_size = env.observation_space.n\n",
        "        \n",
        "        self.eps = 1.0\n",
        "        self.discount_rate = discount_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.build_model()\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.replay_buffer = deque(maxlen=1000) # \n",
        "        \n",
        "    def build_model(self):\n",
        "        tf.reset_default_graph()\n",
        "        self.state_in = tf.placeholder(tf.int32, shape=[None]) # None means any shape\n",
        "        self.action_in = tf.placeholder(tf.int32, shape=[None]) #\n",
        "        self.target_in = tf.placeholder(tf.float32, shape=[None]) #\n",
        "        \n",
        "        self.state = tf.one_hot(self.state_in, depth=self.state_size)\n",
        "        self.action = tf.one_hot(self.action_in, depth=self.action_size)\n",
        "        \n",
        "        self.q_state = tf.layers.dense(self.state, units=self.action_size, name=\"q_table\")\n",
        "        self.q_action = tf.reduce_sum(tf.multiply(self.q_state, self.action), axis=1)\n",
        "        \n",
        "        self.loss = tf.reduce_sum(tf.square(self.target_in - self.q_action))\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "    def get_action(self, state):\n",
        "        q_state = self.sess.run(self.q_state, feed_dict={self.state_in: [state]})\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = super().get_action(state)\n",
        "        return action_random if random.random() < self.eps else action_greedy\n",
        "    \n",
        "    def train(self, experience, batch_size=50):\n",
        "        self.replay_buffer.append(experience) # \n",
        "        samples = random.choices(self.replay_buffer) \n",
        "        state, action, next_state, reward, done = (list(col) for col in zip(experience, *samples)) # \n",
        "        # state, action, next_state, reward, done = ([exp] for exp in experience)\n",
        "        \n",
        "        q_next = self.sess.run(self.q_state, feed_dict={self.state_in: next_state})\n",
        "        q_next[done] = np.zeros([self.action_size])\n",
        "        q_target = reward + self.discount_rate * np.max(q_next, axis=1)\n",
        "        \n",
        "        feed = {self.state_in: state, self.action_in: action, self.target_in: q_target}\n",
        "        self.sess.run(self.optimizer, feed_dict=feed)\n",
        "        \n",
        "        if experience[4]:\n",
        "            self.eps = self.eps * 0.99\n",
        "            \n",
        "    def __del__(self):\n",
        "        self.sess.close()\n",
        "        \n",
        "agent = QNRAgent(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDIRxHqklVBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d49d7dee-95fc-4cdb-9760-05745473eb0d"
      },
      "source": [
        "total_reward = 0\n",
        "for ep in range(100):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        agent.train((state,action,next_state,reward,done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        \n",
        "        print(\"s:\", state, \"a:\", action)\n",
        "        print(\"Episode: {}, Total reward: {}, eps: {}\".format(ep,total_reward,agent.eps))\n",
        "        env.render()\n",
        "        \n",
        "        # Reuse=True... So if we run again, previously learned weights in q-table will be the starting point.\n",
        "        with tf.variable_scope(\"q_table\", reuse=True):\n",
        "            weights = agent.sess.run(tf.get_variable(\"kernel\"))\n",
        "            print(weights)\n",
        "        time.sleep(0.05)\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s: 5 a: 3\n",
            "Episode: 99, Total reward: 9.0, eps: 0.13397967485796175\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "[[ 0.0067195   0.17599882  0.09380232  0.2239123 ]\n",
            " [ 0.01076017  0.13619302 -0.05322899  0.21017969]\n",
            " [ 0.0172868  -0.02394445  0.07531707  0.20565414]\n",
            " [-0.27536845  0.14818554 -0.35104477  0.21138243]\n",
            " [ 0.05686286  0.17754386  0.10101318  0.25296536]\n",
            " [ 0.35862166 -0.11615252  0.3615265  -0.26941258]\n",
            " [-0.15395798  0.16146454  0.04263688  0.0222161 ]\n",
            " [-0.42393816 -0.44041643  0.38130927  0.20673525]\n",
            " [-0.27182758  0.24018717 -0.17501223  0.28538427]\n",
            " [ 0.09451574  0.2660884  -0.04190182  0.24824321]\n",
            " [ 0.00087914  0.25509307  0.17645127  0.00224096]\n",
            " [ 0.10008883  0.04607654  0.11425209 -0.24230176]\n",
            " [ 0.4153232  -0.30978847 -0.0752629  -0.28703558]\n",
            " [-0.12627564  0.14441176 -0.21078219  0.08545002]\n",
            " [ 0.21787913  0.06302729  0.4139699   0.45066664]\n",
            " [-0.12409168 -0.45314607 -0.49180356 -0.53413004]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAhKlLBslVBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKXssbQilVBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J0QA79XlVB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3s7vWlalVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXncYNbrlVB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x5_vX5WlVCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43l1CAVnlVCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soe_gO5hlVCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}